[{"uri":"/FSharp.Control.AsyncSeq/ComparisonWithObservable.html","title":"Comparison with IObservable\n","content":"(*** condition: prepare ***)\n#nowarn \u0022211\u0022\n#I \u0022../src/FSharp.Control.AsyncSeq/bin/Release/netstandard2.1\u0022\n#r \u0022FSharp.Control.AsyncSeq.dll\u0022\n(*** condition: fsx ***)\n#if FSX\n#r \u0022nuget: FSharp.Control.AsyncSeq,{{package-version}}\u0022\n#endif // FSX\n(*** condition: ipynb ***)\n#if IPYNB\n#r \u0022nuget: FSharp.Control.AsyncSeq,{{package-version}}\u0022\n#endif // IPYNB\n\n\n(**\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/fsprojects/FSharp.Control.AsyncSeq/gh-pages?filepath=AsyncSeq.ipynb)\n\n# Comparison with IObservable\n\nBoth \u0060IObservable\u003C\u0027T\u003E\u0060 and \u0060AsyncSeq\u003C\u0027T\u003E\u0060 represent collections of items and both provide similar operations\nfor transformation and composition. The central difference between the two is that the former uses a *synchronous push*\nto a subscriber and the latter uses an *asynchronous pull* by a consumer. \nConsumers of an \u0060IObservable\u003C\u0027T\u003E\u0060 *subscribe* to receive notifications about\nnew items or the end of the sequence. By contrast, consumers of an \u0060AsyncSeq\u003C\u0027T\u003E\u0060 *asynchronously retrieve* subsequent items on their own\nterms. Some domains are more naturally modeled with one or the other, however it is less clear which is a more\nsuitable tool for a specific task. In many cases, a combination of the two provides the optimal solution and \nrestricting yourself to one, while simplifying the programming model, can lead one to view all problems as a nail.\n\nA more specific difference between the two is that \u0060IObservable\u003C\u0027T\u003E\u0060 subscribers have the basic type \u0060\u0027T -\u003E unit\u0060 \nand are therefore inherently synchronous and imperative. The observer can certainly make a blocking call, but this \ncan defeat the purpose of the observable sequence all together. Alternatively, the observer can spawn an operation, but\nthis can break composition because one can no longer rely on the observer returning to determine that it has \ncompleted. With the observable model however, we can model blocking operations through composition on sequences rather\nthan observers.\n\nTo illustrate, let\u0027s try to implement the above Tweet retrieval, filtering and storage workflow using observable sequences.\nSuppose we already have an observable sequence representing tweets \u0060IObservable\u003CTweet\u003E\u0060 and we simply wish \nto filter it and store the resulting tweets. The function \u0060Observable.filter\u0060 allows one to filter observable\nsequences based on a predicate, however in this case it doesn\u0027t quite cut it because the predicate passed to it must\nbe synchronous \u0060\u0027T -\u003E bool\u0060:\n*)\n\nopen System\n\n// At the top of docs/ComparisonWithObservable.fsx\ntype Tweet = { Id: int; Content: string }\nlet filterTweet (tweet: Tweet) = async { return tweet.Content.Contains(\u0022keyword\u0022) }\nlet storeTweet (tweet: Tweet) = printfn \u0022Storing tweet %d\u0022 tweet.Id\n\nlet tweetsObs : IObservable\u003CTweet\u003E =\n  failwith \u0022TODO: create observable\u0022\n\nlet filteredTweetsObs =\n  tweetsObs\n  |\u003E Observable.filter (filterTweet \u003E\u003E Async.RunSynchronously) // blocking IO-call!\n\n(**\nTo remedy the blocking IO-call we can better adapt the filtering function to the \u0060IObservable\u003C\u0027T\u003E\u0060 model. A value\nof type \u0060Async\u003C\u0027T\u003E\u0060 can be modeled as an \u0060IObservable\u003C\u0027T\u003E\u0060 with one element. Suppose that we have \n\u0060Tweet -\u003E IObservable\u003Cbool\u003E\u0060. We can define a few helper operators on observables to allow filtering using\nan asynchronous predicate as follows:\n*)\n\nmodule Observable =\n  \n  /// a |\u003E Async.StartAsTask |\u003E (fun t -\u003E t.ToObservable())\n  let ofAsync (a:Async\u003C\u0027a\u003E) : IObservable\u003C\u0027a\u003E =\n    failwith \u0022TODO\u0022\n\n  /// Observable.SelectMany\n  let bind (f:\u0027a -\u003E IObservable\u003C\u0027b\u003E) (o:IObservable\u003C\u0027a\u003E) : IObservable\u003C\u0027b\u003E =\n    failwith \u0022TODO\u0022\n\n  /// Filter an observable sequence using a predicate producing a observable\n  /// which emits a single boolean value.\n  let filterObs (f:\u0027a -\u003E IObservable\u003Cbool\u003E) : IObservable\u003C\u0027a\u003E -\u003E IObservable\u003C\u0027a\u003E =\n    bind \u003C| fun a -\u003E \n      f a\n      |\u003E Observable.choose (function\n        | true -\u003E Some a\n        | false -\u003E None\n      )\n  \n  /// Filter an observable sequence using a predicate which returns an async\n  /// computation producing a boolean value.\n  let filterAsync (f:\u0027a -\u003E Async\u003Cbool\u003E) : IObservable\u003C\u0027a\u003E -\u003E IObservable\u003C\u0027a\u003E =\n    filterObs (f \u003E\u003E ofAsync)\n\n  /// Maps over an observable sequence using an async-returning function.\n  let mapAsync (f:\u0027a -\u003E Async\u003C\u0027b\u003E) : IObservable\u003C\u0027a\u003E -\u003E IObservable\u003C\u0027b\u003E =\n    bind (f \u003E\u003E ofAsync)\n\nlet filteredTweetsObs\u0027 : IObservable\u003CTweet\u003E =\n  filteredTweetsObs\n  |\u003E Observable.filterAsync filterTweet\n\n\n(**\nWith a little effort, we were able to adapt \u0060IObservable\u003C\u0027a\u003E\u0060 to our needs. Next let\u0027s try implementing the storage of\nfiltered tweets. Again, we can adapt the function \u0060storeTweet\u0060 defined above to the observable model and bind the\nobservable of filtered tweets to it:\n*)\n\nlet storeTweet (tweet: Tweet) : Async\u003Cunit\u003E =\n  failwith \u0022TODO\u0022\n\nlet storedTweetsObs : IObservable\u003Cunit\u003E =\n  filteredTweetsObs\u0027\n  |\u003E Observable.mapAsync storeTweet\n\n(**\nThe observable sequence \u0060storedTweetsObs\u0060 will produces a value each time a filtered tweet is stored. The entire\nworkflow can be expressed as follows:\n*)\n\nlet storedTeetsObs\u0027 : IObservable\u003Cunit\u003E =\n  tweetsObs\n  |\u003E Observable.filterAsync filterTweet\n  |\u003E Observable.mapAsync storeTweet\n\n(**\nOverall, both solutions are succinct and composable and deciding which one to use can ultimately be a matter of preference. \nSome things to consider are the \u0022synchronous push\u0022 vs. \u0022asynchronous pull\u0022 semantics. On the one hand, tweets are pushed based - the consumer has no control \nover their generation. On the other hand, the program at hand will process the tweets on its own terms regardless of how quickly \nthey are being generated. Moreover, the underlying Twitter API will likely utilize a request-reply protocol to retrieve batches of \ntweets from persistent storage. As such, the distinction between \u0022synchronous push\u0022 vs. \u0022asynchronous pull\u0022 becomes less interesting. If the underlying source \nis truly push-based, then one can buffer its output and consume it using an asynchronous sequence. If the underlying source is pull-based, \nthen one can turn it into an observable sequence by first pulling, then pushing. Note however that in a true real-time reactive system, \nnotifications must be pushed immediately without delay.\n\nUpon closer inspection, the consumption approaches between the two models aren\u0027t all too different. While \u0060AsyncSeq\u0060 is based on an asynchronous-pull operation,\nit is usually consumed using an operator such as \u0060AsyncSeq.iterAsync\u0060 as shown above. This is a function of type \n\u0060(\u0027T -\u003E Async\u003Cunit\u003E) -\u003E AsyncSeq\u003C\u0027T\u003E -\u003E Async\u003Cunit\u003E\u0060 where the first argument is a function \u0060\u0027T -\u003E Async\u003Cunit\u003E\u0060 which performs \nsome work on an item of the sequence and is applied repeatedly to subsequent items. In a sense, \u0060iterAsync\u0060 *pushes* values to this \nfunction. The primary difference from observers of observable sequences is the return type \u0060Async\u003Cunit\u003E\u0060 rather than simply \u0060unit\u0060.\n\n## Related Articles\n\n * [Programming with F# asynchronous sequences](http://tomasp.net/blog/async-sequences.aspx/)\n\n*)"},{"uri":"/FSharp.Control.AsyncSeq/AsyncSeq.html","title":"F# Async: FSharp.Control.AsyncSeq\n","content":"(*** condition: prepare ***)\n#nowarn \u0022211\u0022\n#I \u0022../src/FSharp.Control.AsyncSeq/bin/Release/netstandard2.1\u0022\n#r \u0022FSharp.Control.AsyncSeq.dll\u0022\n(*** condition: fsx ***)\n#if FSX\n#r \u0022nuget: FSharp.Control.AsyncSeq,{{package-version}}\u0022\n#endif // FSX\n(*** condition: ipynb ***)\n#if IPYNB\n#r \u0022nuget: FSharp.Control.AsyncSeq,{{package-version}}\u0022\n#endif // IPYNB\n\n\n(**\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/fsprojects/FSharp.Control.AsyncSeq/gh-pages?filepath=AsyncSeq.ipynb)\n\n# F# Async: FSharp.Control.AsyncSeq\n\n\u003E NOTE: There is also the option to use [FSharp.Control.TaskSeq](https://github.com/fsprojects/FSharp.Control.TaskSeq) which has a very similar usage model.\n\nAn AsyncSeq is a sequence in which individual elements are retrieved using an \u0060Async\u0060 computation.\nIt is similar to \u0060seq\u003C\u0027a\u003E\u0060 in that subsequent elements are pulled on-demand.\n\u0060AsyncSeq\u0060 also bears similarity to \u0060IObservable\u003C\u0027a\u003E\u0060 with the former being based on an \u0022asynchronous pull\u0022 and the\nlatter based on a \u0022synchronous push\u0022. Analogs for most operations defined for \u0060Seq\u0060, \u0060List\u0060 and \u0060IObservable\u0060 are also defined for \n\u0060AsyncSeq\u0060. The power of \u0060AsyncSeq\u0060 lies in that many of these operations also have analogs based on \u0060Async\u0060 \nallowing composition of complex asynchronous workflows.\n\n\u003E **v4.0 and later:** \u0060AsyncSeq\u003C\u0027T\u003E\u0060 is a type alias for \u0060System.Collections.Generic.IAsyncEnumerable\u003C\u0027T\u003E\u0060.\n\u003E Any \u0060IAsyncEnumerable\u003C\u0027T\u003E\u0060 value (e.g. from EF Core, ASP.NET Core channels, or \u0060taskSeq { }\u0060) can be used\n\u003E directly as an \u0060AsyncSeq\u003C\u0027T\u003E\u0060 without conversion, and vice-versa.\n\nThe \u0060AsyncSeq\u0060 type is located in the \u0060FSharp.Control.AsyncSeq.dll\u0060 assembly which can be loaded in F# Interactive as follows:\n*)\n\n#r \u0022../../../bin/FSharp.Control.AsyncSeq.dll\u0022\nopen FSharp.Control\n\n(**\n### Generating asynchronous sequences\n\nAn \u0060AsyncSeq\u003C\u0027T\u003E\u0060 can be generated using computation expression syntax much like \u0060seq\u003C\u0027T\u003E\u0060:\n*)\n\nlet async12 = asyncSeq {\n  yield 1\n  yield 2\n}\n\n(**\nAnother way to generate an asynchronous sequence is using the \u0060Async.unfoldAsync\u0060 function. This\nfunction accepts as an argument a function which can generate individual elements based on a state and \nsignal completion of the sequence.\n\nFor example, suppose that you\u0027re writing a program which consumes the Twitter API and stores tweets\nwhich satisfy some criteria into a database. There are several asynchronous request-reply interactions at play - \none to retrieve a batch of tweets from the Twitter API, another to determine whether a tweet satisfies some\ncriteria and finally an operation to write the desired tweet to a database. \n\nGiven the type \u0060Tweet\u0060 to represent an individual tweet, the operation to retrieve a batch of tweets can \nbe modeled with type \u0060int -\u003E Async\u003C(Tweet[] * int) option\u003E\u0060 where the incoming \u0060int\u0060 represents the \noffset into the tweet stream. The asynchronous result is an \u0060Option\u0060 which when \u0060None\u0060 indicates the\nend of the stream, and otherwise contains the batch of retrieved tweets as well as the next offset.\n\nThe above function to retrieve a batch of tweets can be used to generate an asynchronous sequence \nof tweet batches as follows:\n*)\n\ntype Tweet = {\n  user : string\n  message : string\n}\n\nlet getTweetBatch (offset: int) : Async\u003C(Tweet[] * int) option\u003E = \n  failwith \u0022TODO: call Twitter API\u0022\n\nlet tweetBatches : AsyncSeq\u003CTweet[]\u003E = \n  AsyncSeq.unfoldAsync getTweetBatch 0\n\n(**\nThe asynchronous sequence \u0060tweetBatches\u0060 will when iterated, incrementally consume the entire tweet stream.\n\nNext, suppose that the tweet filtering function makes a call to a web service which determines\nwhether a particular tweet is of interest and should be stored in the database. This function can be modeled with\ntype \u0060Tweet -\u003E Async\u003Cbool\u003E\u0060. We can flatten the \u0060tweetBatches\u0060 sequence and then filter it as follows:\n*)\n\nlet filterTweet (t: Tweet) : Async\u003Cbool\u003E =\n  failwith \u0022TODO: call web service\u0022\n\nlet filteredTweets : AsyncSeq\u003CTweet\u003E = \n  tweetBatches\n  |\u003E AsyncSeq.concatSeq // flatten\n  |\u003E AsyncSeq.filterAsync filterTweet // filter\n\n(**\nWhen the resulting sequence \u0060filteredTweets\u0060 is consumed, it will lazily consume the underlying\nsequence \u0060tweetBatches\u0060, select individual tweets and filter them using the function \u0060filterTweets\u0060.\n\nFinally, the function which stores a tweet in the database can be modeled by type \u0060Tweet -\u003E Async\u003Cunit\u003E\u0060.\nWe can store all filtered tweets as follows:\n*)\n\nlet storeTweet (t: Tweet) : Async\u003Cunit\u003E =\n  failwith \u0022TODO: call database\u0022\n\nlet storeFilteredTweets : Async\u003Cunit\u003E =\n  filteredTweets\n  |\u003E AsyncSeq.iterAsync storeTweet\n\n(**\nNote that the value \u0060storeFilteredTweets\u0060 is an asynchronous computation of type \u0060Async\u003Cunit\u003E\u0060. At this point,\nit is a *representation* of the workflow which consists of reading batches of tweets, filtering them and storing them\nin the database. When executed, the workflow will consume the entire tweet stream. The entire workflow can be\nsuccinctly declared and executed as follows:\n*)\n\nAsyncSeq.unfoldAsync getTweetBatch 0\n|\u003E AsyncSeq.concatSeq\n|\u003E AsyncSeq.filterAsync filterTweet\n|\u003E AsyncSeq.iterAsync storeTweet\n|\u003E Async.RunSynchronously\n\n(**\nThe above snippet effectively orchestrates several asynchronous request-reply interactions into a cohesive unit\ncomposed using familiar operations on sequences. Furthermore, it will be executed efficiently in a non-blocking manner.\n*)\n\n(**\n### Comparison with seq\u003C\u0027T\u003E\n\nThe central difference between \u0060seq\u003C\u0027T\u003E\u0060 and \u0060AsyncSeq\u003C\u0027T\u003E\u0060 can be illustrated by introducing the notion of time.\nSuppose that generating subsequent elements of a sequence requires an IO-bound operation. Invoking long \nrunning IO-bound operations from within a \u0060seq\u003C\u0027T\u003E\u0060 will *block* the thread which calls \u0060MoveNext\u0060 on the \ncorresponding \u0060IEnumerator\u0060. An \u0060AsyncSeq\u0060 on the other hand can use facilities provided by the F# \u0060Async\u0060 type to make \nmore efficient use of system resources.\n*)\n\nlet withTime = seq {\n  Thread.Sleep(1000) // calling thread will block\n  yield 1\n  Thread.Sleep(1000) // calling thread will block\n  yield 1\n}\n\nlet withTime\u0027 = asyncSeq {\n  do! Async.Sleep 1000 // non-blocking sleep\n  yield 1\n  do! Async.Sleep 1000 // non-blocking sleep\n  yield 2\n}\n\n(**\nWhen the asynchronous sequence \u0060withTime\u0027\u0060 is iterated, the calls to \u0060Async.Sleep\u0060 won\u0027t block threads. Instead,\nthe *continuation* of the sequence will be scheduled by \u0060Async\u0060 while the calling thread will be free to perform other work. \nOverall, a \u0060seq\u003C\u0027a\u003E\u0060 can be viewed as a special case of an \u0060AsyncSeq\u003C\u0027a\u003E\u0060 where subsequent elements are retrieved\nin a blocking manner.\n\n### Performance Considerations\n\nWhile an asynchronous computation obviates the need to block an OS thread for the duration of an operation, it isn\u0027t always the case\nthat this will improve the overall performance of an application. Note however that an async computation does not *require* a\nnon-blocking operation, it simply allows for it. Also of note is that unlike calling \u0060IEnumerable.MoveNext()\u0060, consuming\nan item from an asynchronous sequence requires several allocations. Usually this is greatly outweighed by the\nbenefits, it can make a difference in some scenarios.\n\n## Related Articles\n\n * [Programming with F# asynchronous sequences](http://tomasp.net/blog/async-sequences.aspx/)\n\n*)"},{"uri":"/FSharp.Control.AsyncSeq/index.html","title":"FSharp.Control.AsyncSeq\n","content":"FSharp.Control.AsyncSeq\n=============\n\nFSharp.Control.AsyncSeq is a collection of asynchronous programming utilities for F#.\n\nAn \u0060AsyncSeq\u003C\u0027T\u003E\u0060 is a sequence in which individual elements are retrieved using an \u0060Async\u0060 computation.\nThe power of \u0060AsyncSeq\u0060 lies in that many of these operations also have analogs based on \u0060Async\u0060 \nallowing composition of complex asynchronous workflows, including compositional cancellation.\n\n\u003E **v4.0:** \u0060AsyncSeq\u003C\u0027T\u003E\u0060 is now a type alias for \u0060System.Collections.Generic.IAsyncEnumerable\u003C\u0027T\u003E\u0060.\n\u003E Values flow freely between \u0060AsyncSeq\u003C\u0027T\u003E\u0060 and \u0060IAsyncEnumerable\u003C\u0027T\u003E\u0060 without any conversion.\n\u003E \u0060AsyncSeq.ofAsyncEnum\u0060 / \u0060AsyncSeq.toAsyncEnum\u0060 are now no-ops and marked obsolete \u2014 remove them.\n\u003E See the [README](https://github.com/fsprojects/FSharp.Control.AsyncSeq#version-40--bcl-iasyncenumerable-compatibility) for migration notes.\n\nAn \u0060AsyncSeq\u003C\u0027a\u003E\u0060 can be generated using computation expression syntax much like \u0060seq\u003C\u0027a\u003E\u0060:\n\n    let oneThenTwo = asyncSeq {\n      yield 1\n      do! Async.Sleep 1000 // non-blocking sleep\n      yield 2\n    }\n\nLearning\n--------------------------\n\n[AsyncSeq](AsyncSeq.html) contains narrative and code samples explaining asynchronous sequences.\n\n[AsyncSeq Examples](AsyncSeqExamples.html) contains examples.\n\n[Terminology](terminology.html) a reference for some of the terminology around F# async.\n \n[Comparison with IObservable](ComparisonWithIObservable.html) contains discussion about the difference between async sequences and IObservables.\n\n[API Reference](reference/index.html) contains automatically generated documentation for all types, modules and functions in the library. \nThis includes additional brief samples on using most of the functions.\n\nContributing and copyright\n--------------------------\n\nThe project is hosted on [GitHub][gh] where you can [report issues][issues], fork \nthe project and submit pull requests. If you\u0027re adding a new public API, please also \nconsider adding [samples][content] that can be turned into a documentation. You might\nalso want to read the [library design notes][readme] to understand how it works.\n\nThe library is available under Apache 2.0 license, which allows modification and \nredistribution for both commercial and non-commercial purposes. For more information see the \n[License file][license] in the GitHub repository. \n\n  [content]: https://github.com/fsprojects/FSharp.Control.AsyncSeq/tree/master/docs/content\n  [gh]: https://github.com/fsprojects/FSharp.Control.AsyncSeq\n  [issues]: https://github.com/fsprojects/FSharp.Control.AsyncSeq/issues\n  [readme]: https://github.com/fsprojects/FSharp.Control.AsyncSeq/blob/master/README.md\n  [license]: https://github.com/fsprojects/FSharp.Control.AsyncSeq/blob/master/LICENSE.txt\n"},{"uri":"/FSharp.Control.AsyncSeq/AsyncSeqExamples.html","title":"F# AsyncSeq Examples\n","content":"(*** condition: prepare ***)\n#nowarn \u0022211\u0022\n#I \u0022../src/FSharp.Control.AsyncSeq/bin/Release/netstandard2.1\u0022\n#r \u0022FSharp.Control.AsyncSeq.dll\u0022\n(*** condition: fsx ***)\n#if FSX\n#r \u0022nuget: FSharp.Control.AsyncSeq,{{package-version}}\u0022\n#endif // FSX\n(*** condition: ipynb ***)\n#if IPYNB\n#r \u0022nuget: FSharp.Control.AsyncSeq,{{package-version}}\u0022\n#endif // IPYNB\n\n(**\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/fsprojects/FSharp.Control.AsyncSeq/gh-pages?filepath=AsyncSeqExamples.ipynb)\n\n# F# AsyncSeq Examples\n\n*)\n\n#r \u0022../../../bin/FSharp.Control.AsyncSeq.dll\u0022\nopen System\nopen FSharp.Control\n\n(**\n\n## Group By\n\n\u0060AsyncSeq.groupBy\u0060 partitions an input sequence into sub-sequences with respect to the specified \u0060projection\u0060 function. This operation is the asynchronous analog to \u0060Seq.groupBy\u0060.\n\n### Example Execution\n\nAn example execution can be depicted visually as follows:\n\n\u0060\u0060\u0060\n--------------------------------------------------\n| source  | a0 | a2 | a3 | a4 | a5 |             |\n| key     | k1 | k2 | k1 | k3 |    |             | \n| result  | k1 * [a1,a3] | k2 * [a2] | k3 * [a4] |\n--------------------------------------------------\n\u0060\u0060\u0060\n\n### Use Case\n\nSuppose we would like to consume a stream of events \u0060AsyncSeq\u003CEvent\u003E\u0060 and perform an operation on each event. The operation on each event is of type \u0060Event -\u003E Async\u003Cunit\u003E\u0060. This can be done as follows:\n\n*)\n\n\ntype Event = {\n  entityId : int64\n  data : string \n}\n\nlet stream : AsyncSeq\u003CEvent\u003E =\n  failwith \u0022undefined\u0022\n\nlet action (e:Event) : Async\u003Cunit\u003E =\n  failwith \u0022undefined\u0022\n\nstream \n|\u003E AsyncSeq.iterAsync action\n\n(**\n\nThe above workflow will read an event from the stream, perform an operation and then read the next event.\nWhile the read operation and the operation on the event are *asynchronous*, the stream is processed *sequentially*.\nIt may be desirable to parallelize the processing of the stream. Suppose that events correspond to some entity, \nsuch as a shopping cart. Events belonging to some shopping cart must be processed in a sequential order, however they\nare independent from events belonging to other shopping carts. Therefore, events belonging to distinct shopping carts\ncan be processed in parallel. Using \u0060AsyncSeq.groupBy\u0060, we can partition the stream into a fixed set of sub-streams \nand then process the sub-streams in parallel using \u0060AsyncSeq.mapAsyncParallel\u0060:\n\n*)\n\nstream\n|\u003E AsyncSeq.groupBy (fun e -\u003E int e.entityId % 4)\n|\u003E AsyncSeq.mapAsyncParallel (snd \u003E\u003E AsyncSeq.iterAsync action)\n|\u003E AsyncSeq.iter ignore\n\n(**\n\n\u0060AsyncSeq.groupBy\u0060 partitions the input sequence into sub-sequences based on a key returned by a projection function. \nThe resulting sub-sequences emit elements when the source sequence emits an element corresponding to the key of the \nsub-sequence. Elements of the resulting sequence are pairs of keys and sub-sequences, in this case \u0060int * AsyncSeq\u003CEvent\u003E\u0060. Since by definition, these sub-sequences are independent, they can be processed in parallel. In fact, the sub-sequences *must* be processed in parallel, because it isn\u0027t possible to complete the processing of a sub-sequence until all elements of the source sequence are exhausted.\n\nTo continue improving the efficiency of our workflow, we can make use of batching. Specifically, we can read the incoming\nevents in batches and we can perform actions on entire batches of events.\n\n*)\n\nlet batchStream : AsyncSeq\u003CEvent[]\u003E =\n  failwith \u0022undefined\u0022\n\nlet batchAction (es:Event[]) : Async\u003Cunit\u003E =\n  failwith \u0022undefined\u0022\n\n(**\n\nOrdering is still important. For example, the batch action could write events into a full-text search index. We would like the full-text search index to be sequentially consistent. As such, the events need to be applied in the order they were emitted. The following workflow has the desired properties:\n\n*)\n\nbatchStream\n|\u003E AsyncSeq.concatSeq // flatten the sequence of event arrays\n|\u003E AsyncSeq.groupBy (fun e -\u003E int e.entityId % 4) // partition into 4 groups\n|\u003E AsyncSeq.mapAsyncParallel (snd \n  \u003E\u003E AsyncSeq.bufferByCountAndTime 500 1000 // buffer sub-sequences\n  \u003E\u003E AsyncSeq.iterAsync batchAction) // perform the batch operation\n|\u003E AsyncSeq.iter ignore\n\n(**\n\nThe above workflow:\n\n1. Reads events in batches.\n2. Flattens the batches.\n3. Partitions the events into mutually exclusive sub-sequences.\n4. Buffers elements of each sub-sequence by time and space.\n5. Processes the sub-sequences in parallel, but individual sub-sequences sequentially.\n\n## Merge\n\n\u0060AsyncSeq.merge\u0060 non-deterministically merges two async sequences into one. It is non-deterministic in the sense that the resulting sequence emits elements whenever *either* input sequence emits a value. Since it isn\u0027t always known which will emit a value first, if at all, the operation is non-deterministic. This operation is in contrast to \u0060AsyncSeq.zip\u0060 which also takes two async sequences and returns a single async sequence, but as opposed to emitting an element when *either* input sequence produces a value, it emits an element when *both* sequences emit a value. This operation is also in contrast to \u0060AsyncSeq.append\u0060 which concatenates two async sequences, emitting all element of one, followed by all elements of the another.\n\n### Example Execution\n\nAn example execution can be depicted visually as follows:\n\n\u0060\u0060\u0060\n-----------------------------------------\n| source1 | t0 |    | t1 |    |    | t2 |\n| source2 |    | u0 |    |    | u1 |    |\n| result  | t0 | u0 | t1 |    | u1 | t2 |\n-----------------------------------------\n\u0060\u0060\u0060\n\n### Use Case\n\nSuppose you wish to perform an operation when either of two async sequences emits an element. One way to do this is two start consuming both async sequences in parallel. If we would like to perform only one operation at a time, we can use \u0060AsyncSeq.merge\u0060 as follows:\n*)\n\n/// Represents an stream emitting elements on a specified interval.\nlet intervalMs (periodMs:int) = asyncSeq {\n  yield DateTime.UtcNow\n  while true do\n    do! Async.Sleep periodMs\n    yield DateTime.UtcNow }\n\nlet either : AsyncSeq\u003CDateTime\u003E =\n  AsyncSeq.merge (intervalMs 20) (intervalMs 30)\n\n(**\n\nThe sequence \u0060either\u0060 emits an element every 20ms and every 30ms.\n\n## Combine Latest\n\n\n\u0060AsyncSeq.combineLatest\u0060 non-deterministically merges two async sequences much like \u0060AsyncSeq.merge\u0060, combining their elements using the specified \u0060combine\u0060 function. The resulting async sequence will only contain elements if both of the source sequences produce at least one element. After combining the first elements the source sequences, this operation emits elements when either source sequence emits an element, passing the newly emitted element as one of the arguments to the \u0060combine\u0060 function, the other being the previously emitted element of that type.\n\n### Example Execution\n\nAn example execution can be depicted visually as follows:\n\n\u0060\u0060\u0060\n----------------------------------------\n| source1 | a0 |    |    | a1 |   | a2 |\n| source2 |    | b0 | b1 |    |   |    |\n| result  |    | c0 | c1 | c2 |   | c3 |\n----------------------------------------\n\nwhere\n\nc0 = f a0 b0\nc1 = f a0 b1\nc2 = f a1 b1\nc3 = f a2 b1\n\u0060\u0060\u0060\n\n### Use Case\n\nSuppose we would like to trigger an operation whenever a change occurs. We can represent changes as an \u0060AsyncSeq\u0060. To gain intuition for this, consider the [Consul](https://www.consul.io/)\nconfiguration management system. It stores configuration information in a tree-like structure. For this purpose of this discussion, it can be thought of as a key-value store\nexposed via HTTP. In addition, \u0060Consul\u0060 supports change notifications using HTTP long-polling - when an HTTP GET request is made to retrieve the value of a key, \nif the request specified a modify-index, \u0060Consul\u0060 won\u0027t respond to the request until a change has occurred *since* the modify-index. We can represent this operation using \nthe type \u0060Key * ModifyIndex -\u003E Async\u003CValue * ModifyIndex\u003E\u0060. Next, we can take this operation and turn it into an \u0060AsyncSeq\u0060 of changes as follows:\n*)\n\ntype Key = string\n\ntype Value = string\n\ntype ModifyIndex = int64\n\nlet longPollKey (key:Key, mi:ModifyIndex) : Async\u003CValue * ModifyIndex\u003E =\n  failwith \u0022undefined\u0022\n\nlet changes (key:Key, mi:ModifyIndex) : AsyncSeq\u003CValue\u003E =\n  AsyncSeq.unfoldAsync \n    (fun (mi:ModifyIndex) -\u003E async {\n      let! value,mi = longPollKey (key, mi)\n      return Some (value,mi) })\n    mi\n\n(**\n\nThe function \u0060changes\u0060 produces an async sequence which emits elements whenever the value corresponding to the key changes. Suppose also that we would like to trigger an operation\nwhenever the key changes or based on a fixed interval. We can represent a fixed interval as an async sequence as follows:\n\nPutting it all together:\n\n*)\n\nlet changesOrInterval : AsyncSeq\u003CValue\u003E =\n  AsyncSeq.combineLatestWith (fun v _ -\u003E v) (changes (\u0022myKey\u0022, 0L)) (intervalMs (1000 * 60))\n\n\n(**\n\nWe can now consume this async sequence and use it to trigger downstream operations, such as updating the configuration of a running program, in flight.\n\n\n## Distinct Until Changed\n\n\u0060AsyncSeq.distinctUntilChanged\u0060 returns an async sequence which returns every element of the source sequence, skipping elements which equal its predecessor.\n\n## Example Execution\n\nAn example execution can be visualized as follows:\n\n\u0060\u0060\u0060\n-----------------------------------\n| source  | a | a | b | b | b | a |\n| result  | a |   | b |   |   | a |\n-----------------------------------\n\u0060\u0060\u0060\n\n### Use Case\n\nSuppose you\u0027re polling a resource which returns status information of a background job.\n\n*)\n\ntype Status = {\n  completed : int\n  finished : bool\n  result : string\n}\n\n/// Gets the status of a job.\nlet status : Async\u003CStatus\u003E =\n  failwith \u0022\u0022\n\nlet statuses : AsyncSeq\u003CStatus\u003E =\n  asyncSeq {\n    let! s = status\n    while true do\n      do! Async.Sleep 1000\n      let! s = status\n      yield s }\n\n(**\n\nThe async sequence \u0060statuses\u0060 will return a status every second. It will return a status regardless of whether the status changed. Assuming the status changes monotonically, we can use \u0060AsyncSeq.distinctUntilChanged\u0060 to transform \u0060statuses\u0060 into an async sequence of distinct statuses:\n\n*)\n\nlet distinctStatuses : AsyncSeq\u003CStatus\u003E =\n  statuses |\u003E AsyncSeq.distinctUntilChanged\n\n\n(**\n\nFinally, we can create a workflow which prints the status every time a change is detected and terminates when the underlying job reaches the \u0060finished\u0060 state:\n\n*)\n\nlet result : Async\u003Cstring\u003E =\n  distinctStatuses\n  |\u003E AsyncSeq.pick (fun st -\u003E \n    printfn \u0022status=%A\u0022 st\n    if st.finished then Some st.result\n    else None)\n\n(**\n\n## Zip\n\n\n\u0060AsyncSeq.zip : AsyncSeq\u003C\u0027a\u003E -\u003E AsyncSeq\u003C\u0027b\u003E -\u003E AsyncSeq\u003C\u0027a * \u0027b\u003E\u0060 takes a pair of sequences and combines them into a sequence of pairs element wise - the first element of one sequence is paired with the first element of the other, and so on. It can be used to pair sequences of related elements into a single sequence. It can also be used to combine a sequence of elements with a sequence of effects. \n\n### Example Execution\n\nAn example execution can be visually depicted as follows:\n\n\u0060\u0060\u0060\n---------------------------------------------\n| source1  |    a1    |    a2    |          |\n| source2  |    b1    |    b2    |    b3    |\n| result   |  a1 * b1 |  a2 * b2 |          | \n---------------------------------------------\n\u0060\u0060\u0060 \n\nNote that the resulting sequence terminates when either input sequence terminates. \n\n### Use Case\n\nSuppose that we have an async sequence of events consumed from a message bus. We would like to process this sequence but we want to ensure that we\u0027re not processing to fast. We can pair the sequence of events with a sequence of durations corresponding to the minimum consumption time. We can do this as follows:\n\n*)\n\nlet events : AsyncSeq\u003CEvent\u003E =\n  failwith \u0022TODO\u0022\n\nlet eventsAtLeastOneSec =\n  AsyncSeq.zipWith \n    (fun a _ -\u003E a) \n    events \n    (AsyncSeq.replicateInfiniteAsync (Async.Sleep 1000))\n\n(**\n\nThe resulting async sequence \u0060eventsAtLeastOneSec\u0060 will emit an element at-most every second. Note that the input sequence of timeouts is infinite - this is to allow the other sequence to have any length since \u0060AsyncSeq.zipWith\u0060 will terminate when either input sequence terminates.\n\n## Buffer by Time and Count\n\n\u0060AsyncSeq.bufferByTimeAndCount\u0060 consumes the input sequence until a specified number of elements are consumed or a timeout expires at which point the resulting sequence emits the buffered of elements, unless no elements have been buffered. It is similar to \u0060AsyncSeq.bufferByCount\u0060 but allows a buffer to be emitted base on a timeout in addition to buffer size. Both are useful for batching inputs before performing an operation. \u0060AsyncSeq.bufferByTimeAndCount\u0060 allows an async workflow to proceed even if there are no inputs received during a certain time period.\n\n### Example Execution\n\nAn example execution can be visually depicted as follows:\n\n\u0060\u0060\u0060\n-------------------------------------------------------\n| source   |  a1 | a2 | a3         | a4      |        |\n| result   |     |    | [a1,a2,a3] |         |  [a4]  |\n-------------------------------------------------------\n\u0060\u0060\u0060\nThe last event \u0060a4\u0060 is emitted after a timeout.\n\n### Use Case\n\nSuppose we\u0027re writing a service which consumes a stream of events and indexes them into full-text search index. We can index each event one by one, however we get a performance improvement if we buffer events into small batches. We can buffer into fixed size batches using \u0060AsyncSeq.bufferByCount\u0060. However, the source event stream may stop emitting events half way through a batch which would leave those events in the buffer until more events arrive. \u0060AsyncSeq.bufferByTimeAndCount\u0060 allows the async workflow to make progress by imposing a bound on how long a non-empty but incomplete buffer can wait more additional items.\n\n*)\n\nlet individualEvents : AsyncSeq\u003CEvent\u003E =\n  failwith \u0022\u0022\n\nlet bufferSize = 100\nlet bufferTimeout = 1000\n\nlet bufferedEvents : AsyncSeq\u003CEvent[]\u003E =\n  events |\u003E AsyncSeq.bufferByCountAndTime bufferSize bufferTimeout   \n"},{"uri":"/FSharp.Control.AsyncSeq/terminology.html","title":"Terminology\n","content":"Terminology\n===========\n\nTerminology is frequently a source of confusion. Often times, terms have different \nmeanings depending on the context, different terms are used to refer to the same \nconcept, and finally not everyone agrees on any of this. The terminology described here \nis scoped to F# and the .NET Framework. The goal is to briefly introduce high level\nconcepts and hopefully alleviate some confusion.\n\n## Thread\n\nA thread in this scope refers to a [managed thread](https://msdn.microsoft.com/en-us/library/6kac2kdh(v=vs.110).aspx). \nA thread is a basic unit to which an OS allocates CPU resources. A *managed* thread usually maps directly to\nan OS thread, however it is possible for a CLR host to override this behavior. A thread has a corresponding context \nconsisting of a pointer to the code being executed as well as stack and register state. \n\nA thread can be viewed as a total ordering of instructions. Instructions executed by different threads\nare partially ordered by causality relationships - in some cases its impossible to tell which ran\nbefore the other. Much of the difficulty in multi-threaded code can be attributed to this lack of \ninformation about ordering.\n\n## Context Switch\n\nA context switch occurs when the OS scheduler decides to change the thread to which it allocates CPU resources.\nIn order to do this, it must save the context of the thread which is giving up use of the CPU and reconstitute\nthe context of the thread which is next in line. Context switches occur for a number of reasons. \nIt occurs naturally as part of preemptive scheduling - threads run for a *time slice* or *quantum* until another\nthread is given a chance to run. Another reason is when a thread explicitly yields its time slice, such as with a call to\n\u0060Thread.Sleep\u0060.\n\n## Synchronous vs. Asynchronous\n\nAn operation is synchronous if the caller must wait for it to complete before making progress. \nMore specifically, the calling *thread* may *block* until the synchronous operation is complete. \nNote that a CPU-bound task exhibits behavior similar to blocking.\n\nAn operation is asynchronous if the request to begin the operation and the result of the operation can\nbe delivered through different channels. This provides a convenient mechanism to encapsulate waiting. In other words, \nan asynchronous operations decouples the means of sending the request from the means of receiving a response. \n\nThis decoupling allows one to in turn decouple the *logical* notion of an operation from the *physical*\ndetails of how it is executed. For example, an asynchronous operation to download a web page is a single \n*logical* operation. Due to its asynchronous nature however, the underlying implementation can start the \noperation on one thread and then deliver the completion notification through a different thread\n(such as an IO completion thread managed by the ThreadPool). In the meantime, the calling thread is free\nto perform other work. In fact, the completion notification can even be handled by the same thread\nas the calling thread.\n\nBy contrast, a synchronous operation will use the calling thread\u0027s context to deliver the completion\nnotification. If the work to be performed is small enough, this can be very efficient. If however the \noperation is long running, the OS will perform a context switch to allow other threads to proceed, and \nthen another context switch to resume the calling thread. Note that synchrony can be viewed as a special\nform of asynchrony.\n\nIn F# asynchrony is represented by the \u0060Async\u0060 type.\n\n## Blocking vs. Non-blocking\n\nA thread is [blocked](http://www.albahari.com/threading/part2.aspx#_Blocking) when its execution\nis paused as it waits for some operation to complete (receiving IO, a lock being released, etc). Once the operation completes, \nthe OS will schedule the thread to resume and continue where it left off.\n\nA non-blocking operation is one that does not prevent the calling thread from making progress. In other words,\nonce an non-blocking operation is started, the calling thread is free to perform other work, such as starting\nyet another operation.\n\nIt is important to remember that when a thread is blocked, the CPU and the system as a whole can still do other work. \nThe issue with blocking is that the specific thread which is blocked can\u0027t do other work and the OS must \nuse resources for thread\u0027s context so that it can context switch continue where it left off once the operating being waited on is complete. \nSince managed threads have a relatively high cost (by default, a .NET thread is allocated 1mb of stack space), this can \nlead to inefficiencies. Non-blocking operations allow one to make more efficient use of system resources.\n\n## Further Reading\n\n * [Managed Threading](https://msdn.microsoft.com/en-us/library/3e8s7xdd%28v=vs.110%29.aspx)\n * [Threading in C#](http://www.albahari.com/threading/)\n * [The Art of Multiprocessor Programming](http://www.amazon.com/Art-Multiprocessor-Programming-Revised-Reprint/dp/0123973376/)"}]